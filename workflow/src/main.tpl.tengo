wf := import("@platforma-sdk/workflow-tengo:workflow")
exec := import("@platforma-sdk/workflow-tengo:exec")
assets:= import("@platforma-sdk/workflow-tengo:assets")
xsv := import("@platforma-sdk/workflow-tengo:pframes.xsv")
pframes := import("@platforma-sdk/workflow-tengo:pframes")
pSpec := import("@platforma-sdk/workflow-tengo:pframes.spec")

pfUMAPConv := import(":pf-umap-conv")
pfTSNEConv := import(":pf-tsne-conv")
pfPCAConv := import(":pf-pca-conv")

pfUMAPBatchConv := import(":pf-umap-batch-conv")
pfTSNEBatchConv := import(":pf-tsne-batch-conv")
pfPCABatchConv := import(":pf-pca-batch-conv")
// pfRawCountsConv := import(":pf-counts-conv")
// pfNormCountsConv := import(":pf-norm-counts-conv")

wf.prepare(func(args){
	metaRefs := {}
	i := 0

	for metaRef in args.covariateRefs {
		metaRefs["metaRef" + i ] = wf.resolve(metaRef, { errIfMissing: true })
		i = i + 1
	}

	return {
		resolvedInput: wf.resolve(args.countsRef, { errIfMissing: true }),
		metaRefs: metaRefs 
	}
})

wf.body(func(args) {

	blockId := wf.blockId().getDataAsJson()
	rawCounts := args.resolvedInput
	inputSpec := rawCounts.spec

	nPCs := args.nPCs
	nNeighbors := args.nNeighbors

	// Set default memory and CPU for imports
    defaultConvMem := "16GiB" // @TODO: set based on the size of the input
    defaultConvCpu := 1 // @TODO: set based on the size of the input

	csvCounts := xsv.exportFrame([rawCounts], "csv", { mem: defaultConvMem, cpu: defaultConvCpu })

	// Always run regular dimensionality reduction
	dimReduction := exec.builder().
		software(assets.importSoftware("@platforma-open/milaboratories.dimensionality-reduction.software:calculate-dimRed")).
		mem("32GiB").
		cpu(16).
		addFile("rawCounts.csv", csvCounts).
		arg("--file_path").arg("rawCounts.csv").
		arg("--output_dir").arg(".").
		arg("--n_pcs").arg(string(nPCs)).
		arg("--n_neighbors").arg(string(nNeighbors)).
		saveFile("umap_results.csv").
		saveFile("tsne_results.csv").
		saveFile("pca_results.csv").
		printErrStreamToStdout().
		saveStdoutContent().
		cache(24 * 60 * 60 * 1000).
		run()

	UMAPDimImportParams := pfUMAPConv.getColumns(blockId, inputSpec)
	UMAPPf := xsv.importFile(dimReduction.getFile("umap_results.csv"), "csv", UMAPDimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

	tSNEDimImportParams := pfTSNEConv.getColumns(blockId, inputSpec)
	tSNEPf := xsv.importFile(dimReduction.getFile("tsne_results.csv"), "csv", tSNEDimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

	PCADimImportParams := pfPCAConv.getColumns(blockId, inputSpec)
	PCAPf := xsv.importFile(dimReduction.getFile("pca_results.csv"), "csv", PCADimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

	// Conditionally run batch correction if covariates are provided
	UMAPHarmonyPf := undefined
	tSNEHarmonyPf := undefined
	PCAHarmonyPf := undefined
	
	if len(args.covariateRefs) > 0 {
		covariates := []
		for _, v in args.metaRefs {
			covariates = append(covariates, v)
		}
		
		csvCovariates := xsv.exportFrame(covariates, "csv", {})

		batchCorrection := exec.builder().
			software(assets.importSoftware("@platforma-open/milaboratories.dimensionality-reduction.software:calculate-batchCorrection")).
			mem("32GiB").
			cpu(16).
			addFile("rawCounts.csv", csvCounts).
			addFile("metadata.csv", csvCovariates).
			arg("--counts").arg("rawCounts.csv").
			arg("--metadata").arg("metadata.csv").
			arg("--output").arg(".").
			saveFile("umap_dimensions.csv").
			saveFile("tsne_dimensions.csv").
			// saveFile("batch_corrected_counts.csv").
			// saveFile("batch_corrected_normalized_counts.csv").
			saveFile("harmony_results.csv").
			printErrStreamToStdout().
			saveStdoutContent().
			cache(24 * 60 * 60 * 1000).
			run()

    	// Process harmony-corrected results
    	UMAPHarmonyDimImportParams := pfUMAPBatchConv.getColumns(blockId, inputSpec)
    	UMAPHarmonyPf = xsv.importFile(batchCorrection.getFile("umap_dimensions.csv"), "csv", UMAPHarmonyDimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

    	tSNEHarmonyDimImportParams := pfTSNEBatchConv.getColumns(blockId, inputSpec)
    	tSNEHarmonyPf = xsv.importFile(batchCorrection.getFile("tsne_dimensions.csv"), "csv", tSNEHarmonyDimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

		// batchCorrectedCountsImportParams := pfRawCountsConv.getColumns(blockId, inputSpec, species)
		// batchCorrectedCountsPf := xsv.importFile(batchCorrection.getFile("batch_corrected_counts.csv"), "csv", batchCorrectedCountsImportParams)

    	PCAHarmonyDimImportParams := pfPCABatchConv.getColumns(blockId, inputSpec)
    	PCAHarmonyPf = xsv.importFile(batchCorrection.getFile("harmony_results.csv"), "csv", PCAHarmonyDimImportParams, {splitDataAndSpec: true, mem: defaultConvMem, cpu: defaultConvCpu})

		// batchCorrectedNormalizedCountsImportParams := pfNormCountsConv.getColumns(blockId, inputSpec, species)
		// batchCorrectedNormalizedCountsPf := xsv.importFile(batchCorrection.getFile("batch_corrected_normalized_counts.csv"), "csv", batchCorrectedNormalizedCountsImportParams)
	}

	// Make trace with informative label
	traceLabel := "(nPCs:" + string(nPCs) + ", nNeighbors:" + string(nNeighbors) + ")"

	// Make trace
	trace := pSpec.makeTrace(inputSpec,
		{
			type: "milaboratories.dimensionality-reduction", 
			id: blockId, importance: 35, 
			label: "Dimensionality Reduction"
		},
		{
			type: "milaboratories.dimensionality-reduction", 
			id: blockId, importance: 30, 
			label: traceLabel
		}
	)
	// Make batch correction trace
	trace_batch := pSpec.makeTrace(inputSpec,
		{
			type: "milaboratories.dimensionality-reduction", 
			id: blockId, importance: 35, 
			label: "Batch Corrected Dimensionality Reduction"
		},
		{
			type: "milaboratories.dimensionality-reduction", 
			id: blockId, importance: 30, 
			label: traceLabel
		}
	)

	//////////// Outputs ////////////

	// Build UMAP pFrame for outputs
	umapOutputPf := pframes.pFrameBuilder()
	for k, v in UMAPPf {
		umapOutputPf.add(k, trace.inject(v.spec), v.data)
	}
	umapOutputPf = umapOutputPf.build()

	// Build tSNE pFrame for outputs
	tsneOutputPf := pframes.pFrameBuilder()
	for k, v in tSNEPf {
		tsneOutputPf.add(k, trace.inject(v.spec), v.data)
	}
	tsneOutputPf = tsneOutputPf.build()

	// Build outputs pFrame
	outputs := {
		UMAPPf: pframes.exportFrame(umapOutputPf),
		tSNEPf: pframes.exportFrame(tsneOutputPf)
	}

	// Conditionally build harmony-corrected outputs pFrame
	if len(args.covariateRefs) > 0 {
		umapHarmonyOutputPf := pframes.pFrameBuilder()
		for k, v in UMAPHarmonyPf {
			umapHarmonyOutputPf.add(k, trace_batch.inject(v.spec), v.data)
		}
		umapHarmonyOutputPf = umapHarmonyOutputPf.build()

		tsneHarmonyOutputPf := pframes.pFrameBuilder()
		for k, v in tSNEHarmonyPf {
			tsneHarmonyOutputPf.add(k, trace_batch.inject(v.spec), v.data)
		}
		tsneHarmonyOutputPf = tsneHarmonyOutputPf.build()

		outputs.UMAPHarmonyPf = pframes.exportFrame(umapHarmonyOutputPf)
		outputs.tSNEHarmonyPf = pframes.exportFrame(tsneHarmonyOutputPf)
	}

	//////////// Exports ////////////

	// Build combined exports pFrame
	exportsPf := pframes.pFrameBuilder()
	i := 0
	
	for pf in [UMAPPf, tSNEPf, PCAPf] {
		for k, v in pf {
			exportsPf.add(string(i) + "_" + k, trace.inject(v.spec), v.data)
			i = i + 1
		}
	}
	
	// Conditionally add harmony-corrected outputs to exports pFrame
	if len(args.covariateRefs) > 0 {
		for pf in [UMAPHarmonyPf, tSNEHarmonyPf] {
			for k, v in pf {
				exportsPf.add(string(i) + "_" + k, trace_batch.inject(v.spec), v.data)
				i = i + 1
			}
		}
	}

	exportsPf = exportsPf.build()

	return {
		outputs: outputs,
		exports: {
			pf: exportsPf
		}
	}
})

